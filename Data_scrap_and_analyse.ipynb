{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f15bfa2-276d-4a39-826b-b392801e7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4d96d5-0679-459e-b23a-a1e82a09c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "!conda install --yes --prefix {sys.prefix} beautifulsoup4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910873b6-7dd8-461c-af41-75a4368bc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\lENOVO\\ML-course\\Scrapping\\env\n",
      "\n",
      "  added / updated specs:\n",
      "    - openpyxl\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    et_xmlfile-1.1.0           |  py312haa95532_1          12 KB\n",
      "    openpyxl-3.1.2             |  py312h2bbff1b_0         650 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         662 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  et_xmlfile         pkgs/main/win-64::et_xmlfile-1.1.0-py312haa95532_1 \n",
      "  openpyxl           pkgs/main/win-64::openpyxl-3.1.2-py312h2bbff1b_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "302cf1d8-76d9-4cc1-a746-fbdf69a14dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c5c08-0c83-4534-9e7e-cc945d73a291",
   "metadata": {},
   "source": [
    "## 1st step\n",
    "#### Installing all the required packages, futher packages in below \n",
    "Now we will read the input file and convert it into necessary data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbd81af9-0d4f-4384-87c7-d556a94fcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel file\n",
    "input_file=pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "204cfce3-8145-4806-88b5-394c8c65be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "271e3140-3311-489e-b4fd-361067ced85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=input_file['URL'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e4bc614e-da8f-405e-b14b-be020b8ec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id=input_file[\"URL_ID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d3f727f7-105f-4a92-952f-5cb50fb0c117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bctech2011', 'bctech2012', 'bctech2013']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_id[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c7065ca-366f-4aaa-b5d1-1f2ecfadd9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39a6f89b-1882-4302-84d4-ea16c6ee3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "results_dir='IDs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c20e4-5b9d-48ec-978c-753b03a10fa5",
   "metadata": {},
   "source": [
    "## 2nd Step\n",
    "### Functions Cell\n",
    "\n",
    "1. Fetch_pages = It collects all the urls of the articles\n",
    "2. parse_pages = It uses html.parse to read the text of the articles\n",
    "3. saving = saves all the articles with url_ID name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d156785-c002-4f1e-9b75-c870848d8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pages(url):\n",
    "    response=requests.get(url)\n",
    "    if response.status_code==200:# 200 status code means accepted \n",
    "        return response.content\n",
    "    else:\n",
    "        print(\"error while retriving the URL\")\n",
    "        return None\n",
    "\n",
    "def parse_pages(content):\n",
    "    soup=bs(content,'html.parser')\n",
    "    # extracting the title \n",
    "    title=soup.find('title').get_text()\n",
    "    #extracting the article text\n",
    "    article_text=soup.find('div',class_='td-post-content tagdiv-type').get_text()\n",
    "    return title,article_text\n",
    "\n",
    "def saving(directory,url_id,title,article_text):\n",
    "\n",
    "    if not os.path.exists(results_dir):\n",
    "        \n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    filename=os.path.join(directory,f\"{url_id}.txt\") # saves the file in text format with url id as name \n",
    "    with open (filename,'w',encoding='utf-8')as file:\n",
    "        file.write(title + '\\n' + article_text)\n",
    "\n",
    "def main(urls,urls_ids):\n",
    "\n",
    "    # loop through each URl in data frame\n",
    "    for url_id,url in zip(urls_ids,urls):\n",
    "        #Fetch the webpage content\n",
    "        content=fetch_pages(url)\n",
    "        if content:\n",
    "            title,article_text=parse_pages(content)\n",
    "            saving(results_dir,url_id, title,article_text)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(urls,url_id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da06105d-8558-4276-beda-d7609be74ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML and AI-based insurance premium model to predict premium to be charged by the insurance company - Blackcoffer Insights'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "res=requests.get(url[0])\n",
    "\n",
    "res.content\n",
    "soup=bs(res.content,'html.parser')\n",
    "title=soup.find('title').get_text()\n",
    "title\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775ff32-6b56-45be-93e0-356793ffa552",
   "metadata": {},
   "source": [
    "## 3rd Step\n",
    "### Text Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac97afa2-3b95-4b9c-9f87-0a8195bcc111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\lENOVO\\ML-course\\Scrapping\\env\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    click-8.1.7                |  py312haa95532_0         200 KB\n",
      "    joblib-1.4.2               |  py312haa95532_0         515 KB\n",
      "    nltk-3.8.1                 |  py312haa95532_0         2.3 MB\n",
      "    regex-2023.10.3            |  py312h2bbff1b_0         350 KB\n",
      "    tqdm-4.66.4                |  py312hfc267ef_0         188 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  click              pkgs/main/win-64::click-8.1.7-py312haa95532_0 \n",
      "  joblib             pkgs/main/win-64::joblib-1.4.2-py312haa95532_0 \n",
      "  nltk               pkgs/main/win-64::nltk-3.8.1-py312haa95532_0 \n",
      "  regex              pkgs/main/win-64::regex-2023.10.3-py312h2bbff1b_0 \n",
      "  tqdm               pkgs/main/win-64::tqdm-4.66.4-py312hfc267ef_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5390ed4-b6fd-430b-aef4-e0b4f2481cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a4cff49-748d-4439-a140-2c5c0bbdfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce9a40-7136-48f6-9f04-b418813d54a1",
   "metadata": {},
   "source": [
    "**Reading all the files for process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07fa3197-8389-4c28-89f3-4af30015c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'IDs'\n",
    "\n",
    "article_texts = []\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(results_dir)\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files:\n",
    "    # Check if the file is a text file\n",
    "    if file.endswith('.txt'):\n",
    "        # Extract URL_ID from the file name (assuming the file name is {URL_ID}.txt)\n",
    "        url_id = os.path.splitext(file)[0]  # Extracts the URL_ID from the file name without extension\n",
    "        \n",
    "        # Read the contents of the file\n",
    "        with open(os.path.join(results_dir, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            article_texts.append(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc76d71-3afe-4285-910c-ce092b67471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article_texts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3354a6-c313-484a-a500-da81cddefd6a",
   "metadata": {},
   "source": [
    "## 4th Step\n",
    "### Processing Steps \n",
    "\n",
    "1. **Cleaning-** removing garbage chars \n",
    "2. **Tokenization-** seperating to words \n",
    "3. **Normalization-** changing all words to lower\n",
    "4. **remove punctuation**\n",
    "5. **Removing Stop WOrds-** Removing all the stopwords which are given  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d4d384a8-1fae-4128-b8a3-dbc1794d9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace newline, non-breaking spaces, and tabs with a space\n",
    "    text = re.sub(r'[\\n\\xa0\\t]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "cleaned_article_texts = [clean_text(text) for text in article_texts]\n",
    "\n",
    "#cleaned_article_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "160a467a-616e-42dc-9ed9-ed136e1aee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a92282ff-2685-4b9a-a36c-da56516047ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10f4e18e-b88f-4606-9090-16e826125082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(cleaned_article_texts):\n",
    "    # Use NLTK's word_tokenize function to split the text into words\n",
    "    return word_tokenize(cleaned_article_texts)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in cleaned_article_texts]\n",
    "\n",
    "# Example: Print the tokenized result for the first cleaned text\n",
    "#print(tokenized_texts[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "47ce623a-84f7-4fef-b347-02a39cfb13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_tokenized=[]\n",
    "\n",
    "for text in cleaned_article_texts:\n",
    "    sentences = sent_tokenize(text)\n",
    "    sen_tokenized.append(sentences)\n",
    "\n",
    "#print(sen_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0cff65cf-d320-4e90-976e-c95ede9f0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens):\n",
    "    # Convert all tokens to lowercase\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "normalized_texts = [normalize(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "#print(normalized_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7157c49-c2ab-4959-9f8b-2fbdd17afeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc=[punc for punc in string.punctuation]\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "888c13e5-7e0c-4428-a17e-290b75ed71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/199.4 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 30.7/199.4 kB 660.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 163.8/199.4 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.4/199.4 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ba6b05d-3693-40b4-9378-867382cff610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Path to your text file\n",
    "file_path = r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Currencies.txt\"\n",
    "\n",
    "# Read a chunk of the file to analyze its encoding\n",
    "with open(file_path, 'rb') as f:\n",
    "    raw_data = f.read(100000)  \n",
    "\n",
    "# Detect the encoding of the file content\n",
    "result = chardet.detect(raw_data)\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a1f14893-b0e3-4949-ac55-082e212c91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stop words files that are provided\n",
    "StopWords_Auditor=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Auditor.txt\",header=None)\n",
    "StopWords_Currencies=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Currencies.txt\",header=None,encoding=\"ISO-8859-1\",sep='\\t' )\n",
    "StopWords_DatesandNumbers=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_DatesandNumbers.txt\",header=None)\n",
    "StopWords_Generic=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Generic.txt\",header=None)\n",
    "StopWords_GenericLong=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_GenericLong.txt\",header=None)\n",
    "StopWords_Geographic=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Geographic.txt\",header=None)\n",
    "StopWords_Names=pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\StopWords\\StopWords_Names.txt\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "29a74a27-2183-41ee-aeb0-f59222e449e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlined Integration Interactive Brokers API with Python for Desktop Trading Application - Blackcoffer Insights Client Background Client A leading fintech firm in the USA Industry Type Finance Products & Services Trading Banking Financing Organization Size 100+ The Problem Integrating the Interactive Brokers API with Python. Creating a user-friendly desktop application interface. Managing concurrent processes and threads. Developing the margin calculator with accurate calculations. Handling data synchronization between TWS and the application. Ensuring security and authentication for TWS access. Providing real-time market data to users. Maintaining a responsive and reliable application. Resolving any potential compatibility issues. Ensuring thorough documentation for users Our Solution Leverage Interactive Brokers API documentation and libraries. Design an intuitive and responsive PyQT5-based desktop UI. Implement threading and preprocessing for concurrent tasks. Develop a robust margin calculator algorithm. Use data synchronization mechanisms provided by TWS. Implement secure authentication for TWS access. Utilize the Interactive Brokers API for real-time market data. Conduct extensive testing and quality assurance. Address compatibility issues through rigorous testing. Document every aspect of the project for users and developers. Solution Architecture Interactive Brokers API for live data and trading access. Python-based server using Django for APIs and data storage. PyQT5-based desktop application for trading dashboard. PostgreSQL database for storing relevant data. Threading and concurrency management for parallel processes. Margin calculator component within the desktop app. Integration with Trader Workstation TWS. Real-time market data feeds from TWS. Responsive front-end using Bootstrap HTML and CSS. Detailed documentation for users and developers. Deliverables Project Github Source Code https//github.com/AjayBidyarthy/Sunil-Misir Tech Stack Tools used Requests Threading and Multiprocessing PyQT5 Language/techniques used Python Models used Django ORM Skills used Python Python Django Python Django REST Framework PyQT5 MultiThreading and MultiProcessing Databases used POstgresql Web Cloud Servers used None What are the technical Challenges Faced during Project Execution Complex integration with the Interactive Brokers API. Designing an efficient and user-friendly desktop interface. Coordinating and managing multiple concurrent threads and processes. Accurate implementation of the margin calculator. Ensuring real-time data synchronization with TWS. Handling authentication and security for TWS access. Providing timely and reliable market data. Resolving compatibility issues on various user machines. Optimizing performance for a responsive application. Documenting every aspect comprehensively. How the Technical Challenges were Solved Extensive research and consultation of Interactive Brokers API documentation. User-centered design principles for the desktop interface. Thorough testing and debugging of multi-threading scenarios. Careful design and testing of margin calculation algorithms. Regular data synchronization checks with TWS. Implementation of secure authentication protocols. Utilization of Interactive Brokers data streaming features. Compatibility testing on various configurations. Profiling and optimization of code for responsiveness. Comprehensive documentation created throughout the development process. Summarize Summarized https//blackcoffer.com/ This project was done by the Blackcoffer Team a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer TeamHere are my contact detailsFirm Name Blackcoffer Pvt. Ltd.Firm Website www.blackcoffer.comFirm Address 4/2 E-Extension Shaym Vihar Phase 1 New Delhi 110043Email ajay@blackcoffer.comSkype asbidyarthyWhatsApp +91 9717367468Telegram @asbidyarthy\n"
     ]
    }
   ],
   "source": [
    "def text_process(text):\n",
    "    # Define punctuation to remove\n",
    "    punc = set([char for char in text if char in [':', ',', '(', ')', '’', '?']])\n",
    "    \n",
    "    # Remove punctuation\n",
    "    nopunc = ''.join([char for char in text if char not in punc])\n",
    "    \n",
    "    # Remove stopwords from each category\n",
    "    nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor.values.flatten()])\n",
    "    nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Currencies.values.flatten()])\n",
    "    nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in StopWords_DatesandNumbers.values.flatten()])\n",
    "    nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Generic.values.flatten()])\n",
    "    \n",
    "    return nopunc\n",
    "\n",
    "\n",
    "\n",
    "processed_texts = [text_process(text) for text in cleaned_article_texts]\n",
    "\n",
    "\n",
    "print(processed_texts[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7225fbaf-fde6-4ea6-ad6f-d08e00c5ed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19652"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_article_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e0275a6-0795-4df5-bcfc-be4b4296a6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19352"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fda453d8-8df6-4b68-b6ca-65177c0066ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the stopwords are removed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226cf54d-2e7c-4d08-baac-d90bf6a4862b",
   "metadata": {},
   "source": [
    "## 5th Step\n",
    "### Variable Operations\n",
    "**Calculating all the variable operations and storing them in a data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c95d591-3c62-4dd7-85b9-49794653a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load positive and negative word lists\n",
    "positive_words = pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\MasterDictionary\\positive-words.txt\", header=None, encoding='ISO-8859-1')\n",
    "negative_words = pd.read_csv(r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\MasterDictionary\\negative-words.txt\", header=None, encoding='ISO-8859-1')\n",
    "\n",
    "# Rename columns to a consistent name\n",
    "positive_words.columns = ['word']\n",
    "negative_words.columns = ['word']\n",
    "\n",
    "# Convert to string type\n",
    "positive_words['word'] = positive_words['word'].astype(str)\n",
    "negative_words['word'] = negative_words['word'].astype(str)\n",
    "\n",
    "positive_words['clean_word']=positive_words['word'].apply(text_process)\n",
    "negative_words['clean_word']=negative_words['word'].apply(text_process)\n",
    "\n",
    "positive_words = positive_words.dropna()\n",
    "negative_words = negative_words.dropna()\n",
    "\n",
    "positive_word_list = positive_words['clean_word'].tolist()\n",
    "negative_word_list = negative_words['clean_word'].tolist()\n",
    "\n",
    "\n",
    "#text=normalized_texts\n",
    "\n",
    "def calculate_posi_neg_scores( text,positive_words, negative_words):\n",
    "\n",
    "    \n",
    "    \n",
    "    #tokens = word_tokenize(text.lower())\n",
    "    #text=normalized_texts\n",
    "\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    \n",
    "    # Calculate positive and negative scores\n",
    "    for token in text:\n",
    "        if token in positive_words:\n",
    "            positive_score += 1\n",
    "        elif token in negative_words:\n",
    "            negative_score += 1\n",
    "    \n",
    "    return positive_score, negative_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "67018dc6-dd8f-48f0-9ed1-879827bcfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list=[]\n",
    "for text in normalized_texts:\n",
    "    positive_score, negative_score = calculate_posi_neg_scores(text, positive_word_list, negative_word_list)\n",
    "    scores_list.append((positive_score, negative_score))\n",
    "\n",
    "score_df = pd.DataFrame(scores_list, columns=['Positive_score', 'Negative_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "876e4646-1854-40af-b05c-0594ac71a322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Positive_score  Negative_score\n",
       "0               141              45\n",
       "1                21               6\n",
       "2                23              10\n",
       "3                13               6\n",
       "4                17               3\n",
       "..              ...             ...\n",
       "142              14              10\n",
       "143              23              20\n",
       "144              10              12\n",
       "145               0               0\n",
       "146               2               0\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304aeca4-649f-4329-9fc7-11e33bf4f3ec",
   "metadata": {},
   "source": [
    "### Polarity score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "230d122e-c79a-41c5-91ef-1dbf35ead58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Positive_score  Negative_score  Polarity_Score\n",
      "0               141              45        0.516129\n",
      "1                21               6        0.555556\n",
      "2                23              10        0.393939\n",
      "3                13               6        0.368421\n",
      "4                17               3        0.700000\n",
      "..              ...             ...             ...\n",
      "142              14              10        0.166667\n",
      "143              23              20        0.069767\n",
      "144              10              12       -0.090909\n",
      "145               0               0        0.000000\n",
      "146               2               0        1.000000\n",
      "\n",
      "[147 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "# Calculate Polarity Score\n",
    "score_df['Polarity_Score'] = (score_df[\"Positive_score\"] - score_df[\"Negative_score\"]) / (score_df[\"Positive_score\"] + score_df[\"Negative_score\"] + 0.000001)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(score_df)\n",
    "\n",
    "#print('polarity_score=', Polarity_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dbd8b-9b09-47c0-9702-b55e134229a1",
   "metadata": {},
   "source": [
    "### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "862c6a16-7055-4445-af77-99b2157d862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "score_df[\"Subjectivity_score\"]=(score_df[\"Positive_score\"] + score_df[\"Negative_score\"] / (len(normalized_texts) + 0.000001 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa7c7b-a3d2-41de-9b36-34dcc5920779",
   "metadata": {},
   "source": [
    "### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89fe37ed-7bc1-40a3-bc0f-f380951daaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentence Length = the number of words / the number of sentences\n",
    "\n",
    "avg_sentence_lengths = [len(normalized_texts) / len(sen_tokenized) for  normalized_texts,sen_tokenized in zip(normalized_texts,sen_tokenized )]\n",
    "\n",
    "score_df[\"Avg_sentence_lenght\"]=avg_sentence_lengths \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127391c-d2ae-4915-b9a1-e8824c455c49",
   "metadata": {},
   "source": [
    "### `Percentage of Complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a84e77b8-e458-4a59-8f54-655fa5db24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of Complex words = the number of complex words / the number of words \n",
    "#Complex words are words in the text that contain more than two syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6b2b97b3-905c-4272-aae4-b3658ee386c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiou\"\n",
    "    word = word.lower().strip()\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def is_complex_word(word):\n",
    "    return count_syllables(word) > 2\n",
    "\n",
    "\n",
    "complex_word_counts = []\n",
    "for tokens in normalized_texts:\n",
    "    complex_word_count = sum(1 for word in tokens if is_complex_word(word))\n",
    "    complex_word_counts.append(complex_word_count)\n",
    "\n",
    "\n",
    "word_counts = [len(tokens) for tokens in normalized_texts]\n",
    "no_words=len(normalized_texts)\n",
    "percent_complex_words = [cw_count / wc if wc > 0 else 0 for cw_count, wc in zip(complex_word_counts, word_counts)]\n",
    "\n",
    "\n",
    "#scores_df['Complex Word Count'] = complex_word_counts\n",
    "score_df['Percent_Complex_Words'] = percent_complex_words\n",
    "\n",
    "#print(scores_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb4023-dd1e-4868-ae3b-4ccc95f9f08b",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2c499492-5f2b-444e-ae4a-2ed82dfb25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "score_df[\"Fog_index\"]=0.4*(score_df[\"Avg_sentence_lenght\"] + score_df[\"Percent_Complex_Words\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87035bbd-5ca1-4857-aa4e-14ba9271261e",
   "metadata": {},
   "source": [
    "### AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8daebab4-611f-4007-ade9-86b1d6fa0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "\n",
    "num_words = [len(normalized_texts) for text in cleaned_article_texts]\n",
    "num_sentences = [len(sentences) for sentences in sen_tokenized]\n",
    "\n",
    "avg_words_per_sentence = [nw / ns if ns > 0 else 0 for nw, ns in zip(num_words, num_sentences)]\n",
    "\n",
    "score_df['Average_Words_Per_Sentence'] = avg_words_per_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9c007-2fef-4767-b8e7-d8bf76ee69eb",
   "metadata": {},
   "source": [
    "### COMPLEX WORD COUNT\n",
    "Already did above in percentage complex word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6eb473f9-a407-4d6c-8f11-ed70e38d16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "score_df['Complex_Word_Count'] = complex_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff2ec0-e8ba-4692-a319-6bf9a1571385",
   "metadata": {},
   "source": [
    "### Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d8ca5376-204d-44b7-b21b-122ffda62059",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Count \n",
    "word_c=[len(count) for count in normalized_texts]\n",
    "score_df[\"Word_count\"]=word_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ea554-3de1-4f58-bfa1-c7a520363f46",
   "metadata": {},
   "source": [
    "### SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1486b11e-b7a0-49bb-a6fd-c1ad4169195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    vowels = 'aeiou'\n",
    "    syllable_count = 0\n",
    "    \n",
    "    # Count the vowels in the word\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            syllable_count += 1\n",
    "    \n",
    "   \n",
    "    if word.endswith('es') or word.endswith('ed'):\n",
    "        syllable_count -= 1\n",
    "    \n",
    "   \n",
    "    if syllable_count <= 0:\n",
    "        syllable_count = 1\n",
    "    \n",
    "    return syllable_count\n",
    "\n",
    "def count_syllables_in_text(text):\n",
    "    words = text\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    return syllable_counts\n",
    "\n",
    "\n",
    "# Print syllable counts for the first article as an example\n",
    "#print(syllable_counts[0])\n",
    "syllable_counts = [sum(count_syllables_in_text(article)) for article in normalized_texts]\n",
    "score_df[\"Syllable_Count\"] = syllable_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182bd43-f410-4785-af99-6e7719d6a1b0",
   "metadata": {},
   "source": [
    "### PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "eff5aa02-7eee-4416-9791-5c5a42023e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def count_personal_pronouns(text):\n",
    "    \n",
    "    pronouns = ['I', 'we', 'my', 'ours', 'us']\n",
    "    \n",
    "    # Initialize the count\n",
    "    pronoun_count = 0\n",
    "    \n",
    "   \n",
    "    for pronoun in pronouns:\n",
    "        \n",
    "        matches = re.findall(r'\\b' + re.escape(pronoun) + r'\\b', text, flags=re.IGNORECASE)\n",
    "        pronoun_count += len(matches)\n",
    "    \n",
    "    return pronoun_count\n",
    "\n",
    "normalized_texts_as_strings = [' '.join(tokens) for tokens in normalized_texts]\n",
    "personal_pronoun_counts = [count_personal_pronouns(article) for article in normalized_texts_as_strings]\n",
    "\n",
    "# Add the counts to the DataFrame\n",
    "score_df[\"Personal_Pronoun_Count\"] = personal_pronoun_counts\n",
    "\n",
    "# Display the DataFrame to check the result\n",
    "#print(score_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36a636-30e1-4932-83d6-faf373bf6899",
   "metadata": {},
   "source": [
    "### AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "91ace093-4262-499b-be9d-e8834cd334a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of the total number of characters in each word/Total number of words\n",
    "\n",
    "normalized_texts_as_strings = [' '.join(tokens) for tokens in normalized_texts]\n",
    "\n",
    "total_chars = sum(len(word) for article_text in normalized_texts_as_strings for word in word_tokenize(article_text))\n",
    "\n",
    "\n",
    "total_words = sum(len(word_tokenize(article_text)) for article_text in normalized_texts_as_strings)\n",
    "\n",
    "\n",
    "average_word_length = total_chars / total_words\n",
    "\n",
    "score_df[\"Avg_word_length\"]=average_word_length\n",
    "#print(f\"Average Word Length: {average_word_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9b7e3641-ad8f-4029-b403-df1eaf00e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9f5a8-d108-4d8d-8374-6edd8a8b527c",
   "metadata": {},
   "source": [
    "### Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f36c22ef-64b4-48de-8d11-166b8e8a174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e704ce01-03d4-4989-9dbe-4ad6bf834d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output=pd.DataFrame(url_id,columns=[\"URL_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cc0e6553-1782-4a0f-a75a-39c2e7f8e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output[\"URL\"]=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "49b04daa-e15a-4f8d-b7bf-cd1a99a99b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Avg_sentence_lenght</th>\n",
       "      <th>Percent_Complex_Words</th>\n",
       "      <th>Fog_index</th>\n",
       "      <th>Average_Words_Per_Sentence</th>\n",
       "      <th>Complex_Word_Count</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Total_Syllable_Count</th>\n",
       "      <th>Syllable_Count</th>\n",
       "      <th>Personal_Pronoun_Count</th>\n",
       "      <th>Avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>45</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>141.306122</td>\n",
       "      <td>17.316384</td>\n",
       "      <td>0.294617</td>\n",
       "      <td>7.051317</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>956</td>\n",
       "      <td>3065</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>2</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>21.040816</td>\n",
       "      <td>10.980769</td>\n",
       "      <td>0.287215</td>\n",
       "      <td>4.512098</td>\n",
       "      <td>2.826923</td>\n",
       "      <td>171</td>\n",
       "      <td>571</td>\n",
       "      <td>1299</td>\n",
       "      <td>1299</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>23.068027</td>\n",
       "      <td>20.114286</td>\n",
       "      <td>0.242898</td>\n",
       "      <td>8.143442</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>172</td>\n",
       "      <td>704</td>\n",
       "      <td>1488</td>\n",
       "      <td>1488</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>13.040816</td>\n",
       "      <td>10.339623</td>\n",
       "      <td>0.284672</td>\n",
       "      <td>4.252637</td>\n",
       "      <td>2.773585</td>\n",
       "      <td>160</td>\n",
       "      <td>548</td>\n",
       "      <td>1224</td>\n",
       "      <td>1224</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>17.020408</td>\n",
       "      <td>23.724138</td>\n",
       "      <td>0.235465</td>\n",
       "      <td>9.589655</td>\n",
       "      <td>5.068966</td>\n",
       "      <td>172</td>\n",
       "      <td>688</td>\n",
       "      <td>1379</td>\n",
       "      <td>1379</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>14.068027</td>\n",
       "      <td>29.114286</td>\n",
       "      <td>0.196271</td>\n",
       "      <td>11.728148</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>210</td>\n",
       "      <td>1019</td>\n",
       "      <td>1917</td>\n",
       "      <td>1917</td>\n",
       "      <td>3</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>23.136054</td>\n",
       "      <td>23.938462</td>\n",
       "      <td>0.163882</td>\n",
       "      <td>9.645822</td>\n",
       "      <td>2.261538</td>\n",
       "      <td>274</td>\n",
       "      <td>1556</td>\n",
       "      <td>2974</td>\n",
       "      <td>2974</td>\n",
       "      <td>7</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>10.081633</td>\n",
       "      <td>32.285714</td>\n",
       "      <td>0.117257</td>\n",
       "      <td>12.962073</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>54</td>\n",
       "      <td>452</td>\n",
       "      <td>812</td>\n",
       "      <td>812</td>\n",
       "      <td>14</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>62.461538</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>156</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>8.478912</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>147</td>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Positive_score  Negative_score  Polarity_Score  Subjectivity_score  \\\n",
       "0               141              45        0.516129          141.306122   \n",
       "1                21               6        0.555556           21.040816   \n",
       "2                23              10        0.393939           23.068027   \n",
       "3                13               6        0.368421           13.040816   \n",
       "4                17               3        0.700000           17.020408   \n",
       "..              ...             ...             ...                 ...   \n",
       "142              14              10        0.166667           14.068027   \n",
       "143              23              20        0.069767           23.136054   \n",
       "144              10              12       -0.090909           10.081633   \n",
       "145               0               0        0.000000            0.000000   \n",
       "146               2               0        1.000000            2.000000   \n",
       "\n",
       "     Avg_sentence_lenght  Percent_Complex_Words  Fog_index  \\\n",
       "0              17.316384               0.294617   7.051317   \n",
       "1              10.980769               0.287215   4.512098   \n",
       "2              20.114286               0.242898   8.143442   \n",
       "3              10.339623               0.284672   4.252637   \n",
       "4              23.724138               0.235465   9.589655   \n",
       "..                   ...                    ...        ...   \n",
       "142            29.114286               0.196271  11.728148   \n",
       "143            23.938462               0.163882   9.645822   \n",
       "144            32.285714               0.117257  12.962073   \n",
       "145           156.000000               0.141026  62.461538   \n",
       "146            21.000000               0.190476   8.478912   \n",
       "\n",
       "     Average_Words_Per_Sentence  Complex_Word_Count  Word_count  \\\n",
       "0                      0.830508                 956        3065   \n",
       "1                      2.826923                 171         571   \n",
       "2                      4.200000                 172         704   \n",
       "3                      2.773585                 160         548   \n",
       "4                      5.068966                 172         688   \n",
       "..                          ...                 ...         ...   \n",
       "142                    4.200000                 210        1019   \n",
       "143                    2.261538                 274        1556   \n",
       "144                   10.500000                  54         452   \n",
       "145                  147.000000                  24         156   \n",
       "146                   21.000000                  29         147   \n",
       "\n",
       "     Total_Syllable_Count  Syllable_Count  Personal_Pronoun_Count  \\\n",
       "0                    6699            6699                       2   \n",
       "1                    1299            1299                       1   \n",
       "2                    1488            1488                       1   \n",
       "3                    1224            1224                       1   \n",
       "4                    1379            1379                       1   \n",
       "..                    ...             ...                     ...   \n",
       "142                  1917            1917                       3   \n",
       "143                  2974            2974                       7   \n",
       "144                   812             812                      14   \n",
       "145                   302             302                       0   \n",
       "146                   279             279                       1   \n",
       "\n",
       "     Avg_word_length  \n",
       "0           5.088347  \n",
       "1           5.088347  \n",
       "2           5.088347  \n",
       "3           5.088347  \n",
       "4           5.088347  \n",
       "..               ...  \n",
       "142         5.088347  \n",
       "143         5.088347  \n",
       "144         5.088347  \n",
       "145         5.088347  \n",
       "146         5.088347  \n",
       "\n",
       "[147 rows x 14 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "efba30e6-d089-494d-acfc-3f6bd7e62ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([Output, score_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "15336774-c7bb-429c-bd91-b785708e1543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Avg_sentence_lenght</th>\n",
       "      <th>Percent_Complex_Words</th>\n",
       "      <th>Fog_index</th>\n",
       "      <th>Average_Words_Per_Sentence</th>\n",
       "      <th>Complex_Word_Count</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Total_Syllable_Count</th>\n",
       "      <th>Syllable_Count</th>\n",
       "      <th>Personal_Pronoun_Count</th>\n",
       "      <th>Avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>141</td>\n",
       "      <td>45</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>141.306122</td>\n",
       "      <td>17.316384</td>\n",
       "      <td>0.294617</td>\n",
       "      <td>7.051317</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>956</td>\n",
       "      <td>3065</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>2</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>21.040816</td>\n",
       "      <td>10.980769</td>\n",
       "      <td>0.287215</td>\n",
       "      <td>4.512098</td>\n",
       "      <td>2.826923</td>\n",
       "      <td>171</td>\n",
       "      <td>571</td>\n",
       "      <td>1299</td>\n",
       "      <td>1299</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>23.068027</td>\n",
       "      <td>20.114286</td>\n",
       "      <td>0.242898</td>\n",
       "      <td>8.143442</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>172</td>\n",
       "      <td>704</td>\n",
       "      <td>1488</td>\n",
       "      <td>1488</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>13.040816</td>\n",
       "      <td>10.339623</td>\n",
       "      <td>0.284672</td>\n",
       "      <td>4.252637</td>\n",
       "      <td>2.773585</td>\n",
       "      <td>160</td>\n",
       "      <td>548</td>\n",
       "      <td>1224</td>\n",
       "      <td>1224</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>17.020408</td>\n",
       "      <td>23.724138</td>\n",
       "      <td>0.235465</td>\n",
       "      <td>9.589655</td>\n",
       "      <td>5.068966</td>\n",
       "      <td>172</td>\n",
       "      <td>688</td>\n",
       "      <td>1379</td>\n",
       "      <td>1379</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bctech2153</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>14.068027</td>\n",
       "      <td>29.114286</td>\n",
       "      <td>0.196271</td>\n",
       "      <td>11.728148</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>210</td>\n",
       "      <td>1019</td>\n",
       "      <td>1917</td>\n",
       "      <td>1917</td>\n",
       "      <td>3</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bctech2154</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>23.136054</td>\n",
       "      <td>23.938462</td>\n",
       "      <td>0.163882</td>\n",
       "      <td>9.645822</td>\n",
       "      <td>2.261538</td>\n",
       "      <td>274</td>\n",
       "      <td>1556</td>\n",
       "      <td>2974</td>\n",
       "      <td>2974</td>\n",
       "      <td>7</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>bctech2155</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>10.081633</td>\n",
       "      <td>32.285714</td>\n",
       "      <td>0.117257</td>\n",
       "      <td>12.962073</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>54</td>\n",
       "      <td>452</td>\n",
       "      <td>812</td>\n",
       "      <td>812</td>\n",
       "      <td>14</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bctech2156</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>62.461538</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>156</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bctech2157</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>8.478912</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>147</td>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>1</td>\n",
       "      <td>5.088347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         URL_ID                                                URL  \\\n",
       "0    bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1    bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2    bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3    bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4    bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "..          ...                                                ...   \n",
       "142  bctech2153  https://insights.blackcoffer.com/population-an...   \n",
       "143  bctech2154  https://insights.blackcoffer.com/google-lsa-ap...   \n",
       "144  bctech2155  https://insights.blackcoffer.com/healthcare-da...   \n",
       "145  bctech2156  https://insights.blackcoffer.com/budget-sales-...   \n",
       "146  bctech2157  https://insights.blackcoffer.com/amazon-buy-bo...   \n",
       "\n",
       "     Positive_score  Negative_score  Polarity_Score  Subjectivity_score  \\\n",
       "0               141              45        0.516129          141.306122   \n",
       "1                21               6        0.555556           21.040816   \n",
       "2                23              10        0.393939           23.068027   \n",
       "3                13               6        0.368421           13.040816   \n",
       "4                17               3        0.700000           17.020408   \n",
       "..              ...             ...             ...                 ...   \n",
       "142              14              10        0.166667           14.068027   \n",
       "143              23              20        0.069767           23.136054   \n",
       "144              10              12       -0.090909           10.081633   \n",
       "145               0               0        0.000000            0.000000   \n",
       "146               2               0        1.000000            2.000000   \n",
       "\n",
       "     Avg_sentence_lenght  Percent_Complex_Words  Fog_index  \\\n",
       "0              17.316384               0.294617   7.051317   \n",
       "1              10.980769               0.287215   4.512098   \n",
       "2              20.114286               0.242898   8.143442   \n",
       "3              10.339623               0.284672   4.252637   \n",
       "4              23.724138               0.235465   9.589655   \n",
       "..                   ...                    ...        ...   \n",
       "142            29.114286               0.196271  11.728148   \n",
       "143            23.938462               0.163882   9.645822   \n",
       "144            32.285714               0.117257  12.962073   \n",
       "145           156.000000               0.141026  62.461538   \n",
       "146            21.000000               0.190476   8.478912   \n",
       "\n",
       "     Average_Words_Per_Sentence  Complex_Word_Count  Word_count  \\\n",
       "0                      0.830508                 956        3065   \n",
       "1                      2.826923                 171         571   \n",
       "2                      4.200000                 172         704   \n",
       "3                      2.773585                 160         548   \n",
       "4                      5.068966                 172         688   \n",
       "..                          ...                 ...         ...   \n",
       "142                    4.200000                 210        1019   \n",
       "143                    2.261538                 274        1556   \n",
       "144                   10.500000                  54         452   \n",
       "145                  147.000000                  24         156   \n",
       "146                   21.000000                  29         147   \n",
       "\n",
       "     Total_Syllable_Count  Syllable_Count  Personal_Pronoun_Count  \\\n",
       "0                    6699            6699                       2   \n",
       "1                    1299            1299                       1   \n",
       "2                    1488            1488                       1   \n",
       "3                    1224            1224                       1   \n",
       "4                    1379            1379                       1   \n",
       "..                    ...             ...                     ...   \n",
       "142                  1917            1917                       3   \n",
       "143                  2974            2974                       7   \n",
       "144                   812             812                      14   \n",
       "145                   302             302                       0   \n",
       "146                   279             279                       1   \n",
       "\n",
       "     Avg_word_length  \n",
       "0           5.088347  \n",
       "1           5.088347  \n",
       "2           5.088347  \n",
       "3           5.088347  \n",
       "4           5.088347  \n",
       "..               ...  \n",
       "142         5.088347  \n",
       "143         5.088347  \n",
       "144         5.088347  \n",
       "145         5.088347  \n",
       "146         5.088347  \n",
       "\n",
       "[147 rows x 16 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "dfb1b2c2-2da4-4bfd-9b01-49cedeeb1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\output_Data_file.csv\"\n",
    "merged_df.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d555d06-1594-4f81-a059-5060b69ca92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a1e828de-eca8-4868-a2ba-08f64f1e2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_file=r\"C:\\Users\\lENOVO\\ML-course\\Scrapping\\Output Data Structure.xlsx\"\n",
    "\n",
    "Output_Data_Structure=pd.read_excel(ex_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "24e6c476-8869-4aa8-b727-855cf88f4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_df=score_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fe034-1866-4d96-a63a-eb31dc2be0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
